{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color= orange> **Name : Sudeesh Kumar V** <font color= orange> \n",
    " \n",
    "# **Rollno : CH.EN.U4AIE22059**\n",
    "# **22AIE315 - NLP**\n",
    "# **Lab - 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color= YELLOW> Introduction to NLP and spaCy <font color= orange> \n",
    "\n",
    "## *AIM*\n",
    "<font color = \"snow\"> \n",
    "\n",
    "- ### To explore various Natural Language Processing (NLP) operations and spaCy functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.3-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.11-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.10-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.2-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.25.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sudeesh kumar v\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.27.1-cp311-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading blis-1.0.2-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-2.0.2-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sudeesh kumar v\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.0)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.8.3-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.2 MB 929.6 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.8/12.2 MB 1.2 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.8/12.2 MB 1.2 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.0/12.2 MB 1.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.3/12.2 MB 1.1 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 1.6/12.2 MB 1.1 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.8/12.2 MB 1.1 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 2.1/12.2 MB 1.1 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.4/12.2 MB 1.1 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 2.6/12.2 MB 1.1 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 2.9/12.2 MB 1.2 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 3.1/12.2 MB 1.2 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.4/12.2 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 3.7/12.2 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 3.9/12.2 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.2/12.2 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.5/12.2 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.5/12.2 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.0/12.2 MB 1.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.5/12.2 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.8/12.2 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 6.0/12.2 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.3/12.2 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.6/12.2 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.8/12.2 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.1/12.2 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.1/12.2 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.3/12.2 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.6/12.2 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.9/12.2 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 8.1/12.2 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.4/12.2 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.7/12.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.9/12.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.2/12.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.4/12.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.4/12.2 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.0/12.2 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.2/12.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.5/12.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.7/12.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.0/12.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.5/12.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.10-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.11-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Downloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp311-none-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.0 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.0/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.0-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/632.6 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 524.3/632.6 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 632.6/632.6 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.2-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 672.2 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 0.8/1.5 MB 884.1 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 915.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 987.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.0 MB/s eta 0:00:00\n",
      "Downloading numpy-2.0.2-cp311-cp311-win_amd64.whl (15.9 MB)\n",
      "   ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/15.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/15.9 MB 1.2 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.8/15.9 MB 1.2 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 1.0/15.9 MB 1.2 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 1.0/15.9 MB 1.2 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.3/15.9 MB 1.2 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.6/15.9 MB 1.2 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.8/15.9 MB 1.2 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 2.1/15.9 MB 1.2 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 2.4/15.9 MB 1.2 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.6/15.9 MB 1.2 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.9/15.9 MB 1.1 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 3.1/15.9 MB 1.2 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 3.4/15.9 MB 1.2 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 3.7/15.9 MB 1.2 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 3.9/15.9 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 4.2/15.9 MB 1.2 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 4.5/15.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 4.7/15.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 5.0/15.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 5.0/15.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 5.5/15.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 5.5/15.9 MB 1.2 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 6.0/15.9 MB 1.2 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 6.0/15.9 MB 1.2 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 6.3/15.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 6.6/15.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 6.8/15.9 MB 1.2 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 7.1/15.9 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 7.3/15.9 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 7.6/15.9 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 7.9/15.9 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 8.1/15.9 MB 1.2 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 8.4/15.9 MB 1.2 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 8.7/15.9 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 8.9/15.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 9.2/15.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 9.2/15.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 9.2/15.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 9.2/15.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 9.2/15.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 9.2/15.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 9.4/15.9 MB 1.0 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 9.7/15.9 MB 1.0 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 10.0/15.9 MB 1.0 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 10.2/15.9 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 10.5/15.9 MB 1.1 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 10.7/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 11.0/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 11.0/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 11.3/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 11.5/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 11.8/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 12.1/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 12.3/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 12.6/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 12.8/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 13.1/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 13.4/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 13.6/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 13.9/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 13.9/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.2/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.4/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 14.7/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 14.9/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.5/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.9/15.9 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.0.2-cp311-cp311-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 85.6 kB/s eta 0:01:09\n",
      "   --- ------------------------------------ 0.5/6.3 MB 85.6 kB/s eta 0:01:09\n",
      "   --- ------------------------------------ 0.5/6.3 MB 85.6 kB/s eta 0:01:09\n",
      "   --- ------------------------------------ 0.5/6.3 MB 85.6 kB/s eta 0:01:09\n",
      "   --- ------------------------------------ 0.5/6.3 MB 85.6 kB/s eta 0:01:09\n",
      "   --- ------------------------------------ 0.5/6.3 MB 85.6 kB/s eta 0:01:09\n",
      "   --- ------------------------------------ 0.5/6.3 MB 85.6 kB/s eta 0:01:09\n",
      "   ---- ----------------------------------- 0.8/6.3 MB 114.1 kB/s eta 0:00:49\n",
      "   ------ --------------------------------- 1.0/6.3 MB 164.5 kB/s eta 0:00:33\n",
      "   -------- ------------------------------- 1.3/6.3 MB 209.0 kB/s eta 0:00:25\n",
      "   --------- ------------------------------ 1.6/6.3 MB 251.9 kB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 290.1 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 2.1/6.3 MB 325.3 kB/s eta 0:00:14\n",
      "   -------------- ------------------------- 2.4/6.3 MB 358.9 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 383.2 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 383.2 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 2.9/6.3 MB 406.2 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 3.1/6.3 MB 434.2 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 3.4/6.3 MB 458.6 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 3.4/6.3 MB 458.6 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 3.9/6.3 MB 497.6 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 4.2/6.3 MB 518.8 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 535.7 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 553.8 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.0/6.3 MB 572.0 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 5.2/6.3 MB 588.1 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.2/6.3 MB 588.1 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 5.8/6.3 MB 618.1 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 618.1 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.0/6.3 MB 629.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 647.1 kB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 882.6 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.0/5.4 MB 1.2 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.0/5.4 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 1.8/5.4 MB 1.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.1/5.4 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 2.6/5.4 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 2.6/5.4 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 2.6/5.4 MB 1.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 2.9/5.4 MB 969.8 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 3.4/5.4 MB 818.3 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 3.7/5.4 MB 816.8 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 841.7 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 4.2/5.4 MB 858.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 4.5/5.4 MB 874.4 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 891.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 896.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 913.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 915.9 kB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, shellingham, pydantic-core, numpy, murmurhash, marisa-trie, cloudpathlib, catalogue, annotated-types, srsly, pydantic, preshed, language-data, blis, typer, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "Successfully installed annotated-types-0.7.0 blis-1.0.2 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.10 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.11 numpy-2.0.2 preshed-3.0.9 pydantic-2.10.3 pydantic-core-2.27.1 shellingham-1.5.4 smart-open-7.0.5 spacy-3.8.3 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.0 thinc-8.3.2 typer-0.15.1 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorboard (C:\\Users\\Sudeesh Kumar V\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (C:\\Users\\Sudeesh Kumar V\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (C:\\Users\\Sudeesh Kumar V\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.1.1 requires numpy<2.0,>=1.16; python_version <= \"3.11\", but you have numpy 2.0.2 which is incompatible.\n",
      "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
      "mlflow 2.8.0 requires numpy<2, but you have numpy 2.0.2 which is incompatible.\n",
      "numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.2 which is incompatible.\n",
      "scikit-learn 1.3.1 requires numpy<2.0,>=1.17.3, but you have numpy 2.0.2 which is incompatible.\n",
      "scipy 1.11.2 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.0.2 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.0.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"gold\">\n",
    "\n",
    "## The Doc Object for Processed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sudeesh kumar v\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.2)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\sudeesh kumar v\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sudeesh kumar v\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorboard (C:\\Users\\Sudeesh Kumar V\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (C:\\Users\\Sudeesh Kumar V\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (C:\\Users\\Sudeesh Kumar V\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.7 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     --- ------------------------------------ 1.0/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 1.3 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 1.3 MB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.3 MB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.3 MB/s eta 0:00:09\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     -------- ------------------------------- 2.6/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------- -------------------------- 4.2/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------- -------------------------- 4.5/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     -------------- ------------------------- 4.7/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 5.0/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 5.8/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 6.0/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 6.0/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 6.3/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 6.6/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 6.8/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 6.8/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 6.8/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 6.8/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 6.8/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 1.0 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 1.0 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 1.0 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 7.9/12.8 MB 1.0 MB/s eta 0:00:05\n",
      "     ------------------------- -------------- 8.1/12.8 MB 1.0 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 8.4/12.8 MB 1.0 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 8.7/12.8 MB 1.0 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 8.9/12.8 MB 1.0 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 1.0 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 1.1 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 9.7/12.8 MB 1.1 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 10.0/12.8 MB 1.1 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 10.2/12.8 MB 1.1 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 10.2/12.8 MB 1.1 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 10.7/12.8 MB 1.1 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 1.1 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 1.1 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 11.5/12.8 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 11.8/12.8 MB 1.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Sudeesh Kumar V\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Sudeesh Kumar V\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Sudeesh Kumar V\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x21061944c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "introduction_doc = nlp(\" This tutorial is about Natural Language Processing in Python, using libraries such as spaCy and NLTK\")\n",
    "type(introduction_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'This',\n",
       " 'tutorial',\n",
       " 'is',\n",
       " 'about',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'in',\n",
       " 'Python',\n",
       " ',',\n",
       " 'using',\n",
       " 'libraries',\n",
       " 'such',\n",
       " 'as',\n",
       " 'spaCy',\n",
       " 'and',\n",
       " 'NLTK']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in introduction_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'iam', 'Sudeesh', 'Kumar', 'and', 'my', 'roll', 'number', 'is', 'CH.EN.U4AIE22059']\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Specify the full path to the file (use raw string or escape backslashes)\n",
    "file_name = r\"introduction.txt\" \n",
    "\n",
    "# Load and process the file if it exists\n",
    "file_path = pathlib.Path(file_name)\n",
    "if file_path.exists():\n",
    "    introduction_doc = nlp(file_path.read_text(encoding=\"utf-8\"))\n",
    "    print([token.text for token in introduction_doc])\n",
    "else:\n",
    "    print(f\"File '{file_name}' not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"orange\">\n",
    "\n",
    "# 2.a) Sentence Detection\n",
    "<font color = \"snow\">\n",
    "\n",
    "- ### Sentence detection is the process of locating where sentences start and end in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "about_doc = nlp(about_text)\n",
    "sentences = list(about_doc.sents)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Proto is a Python ... \n",
      "He is interested in learning ... \n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(f\"{sentence[:5]} ... \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipsis_text = (\n",
    "    \"Gus, can you, ... never mind, I forgot\"\n",
    "    \" what I was saying. So, do you think\"\n",
    "    \" we should ... \"\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[ :- 1]:\n",
    "        if token.text == \" ... \":\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom_boundaries(doc)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ellipsis_doc = custom_nlp(ellipsis_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus, can you, ... never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n"
     ]
    }
   ],
   "source": [
    "for sentence in custom_ellipsis_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"orange\">\n",
    "\n",
    "# **Tokens in spaCY**\n",
    "<font color = \"snow\">\n",
    "\n",
    "- ### In spaCy, tokens are the individual building blocks of text, such as words, punctuation, or symbols, created during tokenization. SpaCy provides a Token object for each, allowing access to linguistic features like part-of-speech tags, dependencies, and lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus 0\n",
      "Proto 4\n",
      "is 10\n",
      "a 13\n",
      "Python 15\n",
      "developer 22\n",
      "currently 32\n",
      "working 42\n",
      "for 50\n",
      "a 54\n",
      "London 56\n",
      "- 62\n",
      "based 63\n",
      "Fintech 69\n",
      "company 77\n",
      ". 84\n",
      "He 86\n",
      "is 89\n",
      "interested 92\n",
      "in 103\n",
      "learning 106\n",
      "Natural 115\n",
      "Language 123\n",
      "Processing 132\n",
      ". 142\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "for token in about_doc:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Whitespace  Is Alphanumeric?Is Punctuation?   Is Stop Word?\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "f'{\"Text with Whitespace\":22}'\n",
    "f'{\"Is Alphanumeric?\":15}'\n",
    "f'{\"Is Punctuation?\":18}'\n",
    "f'{\"Is Stop Word?\"}'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus                   True           False             False\n",
      "Proto                 True           False             False\n",
      "is                    True           False             True\n",
      "a                     True           False             True\n",
      "Python                True           False             False\n",
      "developer             True           False             False\n",
      "currently             True           False             False\n",
      "working               True           False             False\n",
      "for                   True           False             True\n",
      "a                     True           False             True\n",
      "London                True           False             False\n",
      "-                     False          True              False\n",
      "based                 True           False             False\n",
      "Fintech               True           False             False\n",
      "company               True           False             False\n",
      ".                     False          True              False\n",
      "He                    True           False             True\n",
      "is                    True           False             True\n",
      "interested            True           False             False\n",
      "in                    True           False             True\n",
      "learning              True           False             False\n",
      "Natural               True           False             False\n",
      "Language              True           False             False\n",
      "Processing            True           False             False\n",
      ".                     False          True              False\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "    print(\n",
    "f'{str(token.text_with_ws):22}'\n",
    "f'{str(token.is_alpha):15}'\n",
    "f'{str(token.is_punct):18}'\n",
    "f'{str(token.is_stop)}'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London@based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'London', '-', 'based', 'Fintech', 'company']\n"
     ]
    }
   ],
   "source": [
    "tokens_to_print = [token.text for token in about_doc[8:15]]\n",
    "print(tokens_to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'London', '@', 'based', 'Fintech', 'company']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
    "suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
    "\n",
    "custom_infixes = [r\"@\"]\n",
    "infix_re = spacy.util.compile_infix_regex(\n",
    "list(custom_nlp.Defaults.infixes) + custom_infixes\n",
    "\n",
    ")\n",
    "\n",
    "custom_nlp.tokenizer = Tokenizer(\n",
    "    custom_nlp.vocab,\n",
    "    prefix_search=prefix_re.search,\n",
    "    suffix_search=suffix_re.search,\n",
    "    infix_finditer=infix_re.finditer,\n",
    "    token_match=None,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "custom_about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London@based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "\n",
    "custom_tokenizer_about_doc = custom_nlp(custom_about_text)\n",
    "\n",
    "print([token.text for token in custom_tokenizer_about_doc[8:15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p><font color=\"orange\">2.b) Stop Words</font></p>\n",
    "\n",
    "<font color = \"snow\">\n",
    "\n",
    "- ### *Stop words are common words like \"is,\" \"the,\" \"and,\" or \"in\" that are typically filtered out in NLP tasks because they carry little meaningful information. Removing them helps focus on significant words for tasks like text analysis or search.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everyone\n",
      "bottom\n",
      "across\n",
      "either\n",
      "around\n",
      "'re\n",
      "hereafter\n",
      "nowhere\n",
      "only\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_stopwords = spacy. lang.en. stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)\n",
    "\n",
    "for stop_word in list(spacy_stopwords) [:10]:\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"orange\">\n",
    "\n",
    "#  Part-of-Speech (POS) Tagging\n",
    "\n",
    "<font color = \"snow\">\n",
    "\n",
    "- ### Part-of-Speech (POS) Tagging in NLP is the process of assigning grammatical categories, such as nouns, verbs, adjectives, or adverbs, to words in a sentence based on their context. It helps in understanding sentence structure and is crucial for syntactic and semantic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gus', 'Proto', 'Python', 'developer', 'currently', 'working', 'London', '-', 'based', 'Fintech', 'company', '.', 'interested', 'learning', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "custom_about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "about_doc = nlp(custom_about_text)\n",
    "\n",
    "filtered_tokens = [token.text for token in about_doc if not token.is_stop]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p><font color=\"orange\">2.c)Lemmatization</font></p>\n",
    "\n",
    "<font color =\"snow\">\n",
    "\n",
    "- ### *Lemmatization in NLP is the process of reducing words to their base or root form (lemma) while considering the context, such as converting \"running\" to \"run\" or \"better\" to \"good.\" It helps normalize text for analysis while preserving meaning.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  is: be\n",
      "          Processing: processing\n",
      "                  He: he\n",
      "               keeps: keep\n",
      "          organizing: organize\n",
      "             meetups: meetup\n",
      "               talks: talk\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "conference_help_text = (\n",
    "    \"Gus is helping organize a developer\"\n",
    "    \" conference on Applications of Natural Language\"\n",
    "    \" Processing. He keeps organizing local Python meetups\"\n",
    "    \" and several internal talks at his workplace.\"\n",
    "\n",
    ")\n",
    "\n",
    "conference_help_doc = nlp(conference_help_text)\n",
    "\n",
    "for token in conference_help_doc:\n",
    "    if str(token) != str(token.lemma_):\n",
    "        print(f\"{str(token):>20}: {str(token.lemma_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p><font color=\"orange\">2.d) Word Frequency</font></p>\n",
    "\n",
    "<font color = \"snow\">\n",
    "\n",
    "- ### *Word frequency refers to the count of how often each word appears in a given text or corpus. It is commonly used in NLP tasks like text analysis, keyword extraction, and building language models to understand word importance and usage patterns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "complete_text = (\n",
    "\"Gus Proto is a Python developer currently\"\n",
    "\" working for a London-based Fintech company. He is\"\n",
    "\" interested in learning Natural Language Processing.\"\n",
    "\" There is a developer conference happening on 21 July\"\n",
    "' 2019 in London. It is titled \"Applications of Natural'\n",
    "' Language Processing\". There is a helpline number'\n",
    "\" available at +44-1234567891. Gus is helping organize it.\"\n",
    "\" He keeps organizing local Python meetups and several\"\n",
    "\" internal talks at his workplace. Gus is also presenting\"\n",
    "' a talk. The talk will introduce the reader about \"Use'\n",
    "' cases of Natural Language Processing in Fintech\".'\n",
    "\" Apart from his work, he is very passionate about music.\"\n",
    "\" Gus is learning to play the Piano. He has enrolled\"\n",
    "\" himself in the weekend batch of Great Piano Academy.\"\n",
    "\" Great Piano Academy is situated in Mayfair or the City\"\n",
    "\" of London and has world-class piano instructors.\"\n",
    "\n",
    ")\n",
    "\n",
    "complete_doc = nlp(complete_text)\n",
    "\n",
    "words = [\n",
    "    token.text\n",
    "    for token in complete_doc\n",
    "        if not token. is_stop and not token. is_punct\n",
    "]   \n",
    "print(Counter(words).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 10), ('a', 5), ('in', 5), ('Gus', 4), ('of', 4)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(\n",
    "    [token. text for token in complete_doc if not token. is_punct]\n",
    ").most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"orange\">\n",
    "\n",
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "TOKEN: Gus\n",
      "\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: Proto\n",
      "\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: is\n",
      "\n",
      "TAG: VBZ        POS: AUX\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: a\n",
      "\n",
      "TAG: DT         POS: DET\n",
      "EXPLANATION: determiner\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: Python\n",
      "\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: developer\n",
      "\n",
      "TAG: NN         POS: NOUN\n",
      "EXPLANATION: noun, singular or mass\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: currently\n",
      "\n",
      "TAG: RB         POS: ADV\n",
      "EXPLANATION: adverb\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: working\n",
      "\n",
      "TAG: VBG        POS: VERB\n",
      "EXPLANATION: verb, gerund or present participle\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: for\n",
      "\n",
      "TAG: IN         POS: ADP\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: a\n",
      "\n",
      "TAG: DT         POS: DET\n",
      "EXPLANATION: determiner\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: London\n",
      "\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: -\n",
      "\n",
      "TAG: HYPH       POS: PUNCT\n",
      "EXPLANATION: punctuation mark, hyphen\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: based\n",
      "\n",
      "TAG: VBN        POS: VERB\n",
      "EXPLANATION: verb, past participle\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: Fintech\n",
      "\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: company\n",
      "\n",
      "TAG: NN         POS: NOUN\n",
      "EXPLANATION: noun, singular or mass\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: .\n",
      "\n",
      "TAG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: He\n",
      "\n",
      "TAG: PRP        POS: PRON\n",
      "EXPLANATION: pronoun, personal\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: is\n",
      "\n",
      "TAG: VBZ        POS: AUX\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: interested\n",
      "\n",
      "TAG: JJ         POS: ADJ\n",
      "EXPLANATION: adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: in\n",
      "\n",
      "TAG: IN         POS: ADP\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: learning\n",
      "\n",
      "TAG: VBG        POS: VERB\n",
      "EXPLANATION: verb, gerund or present participle\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: Natural\n",
      "\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: Language\n",
      "\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: Processing\n",
      "\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "\n",
      "\n",
      "TOKEN: .\n",
      "\n",
      "TAG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "about_text = (\n",
    "\"Gus Proto is a Python developer currently\"\n",
    "\" working for a London-based Fintech\"\n",
    "\" company. He is interested in learning\"\n",
    "\" Natural Language Processing.\"\n",
    "\n",
    ")\n",
    "\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "for token in about_doc:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "\n",
    "\n",
    "TOKEN: {str(token)}\n",
    "\n",
    "TAG: {str(token.tag_):10} POS: {token.pos_}\n",
    "EXPLANATION: {spacy.explain(token.tag_)}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "\n",
    "for token in about_doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns. append(token)\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adjectives.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[developer, company]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[interested]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p><font color=\"orange\">2.e) Visulaization: Using displaCy</font></p>\n",
    "\n",
    "<font color = \"snow\">\n",
    "\n",
    "- ### *Visualization in spaCy using displaCy provides an interactive way to explore linguistic structures. It supports:*\n",
    "1) Dependency Parsing: Displays syntactic dependencies as arcs over tokens in a sentence.\n",
    "2) Named Entity Recognition (NER): Highlights entities in text with labels for entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9aef1e1900cf4676b1de52c7f678fb30-0\" class=\"displacy\" width=\"850\" height=\"237.0\" direction=\"ltr\" style=\"max-width: none; height: 237.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">interested</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">Natural</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">Language</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Processing.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9aef1e1900cf4676b1de52c7f678fb30-0-0\" stroke-width=\"2px\" d=\"M70,102.0 C70,52.0 145.0,52.0 145.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9aef1e1900cf4676b1de52c7f678fb30-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,104.0 L62,92.0 78,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9aef1e1900cf4676b1de52c7f678fb30-0-1\" stroke-width=\"2px\" d=\"M170,102.0 C170,52.0 245.0,52.0 245.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9aef1e1900cf4676b1de52c7f678fb30-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245.0,104.0 L253.0,92.0 237.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9aef1e1900cf4676b1de52c7f678fb30-0-2\" stroke-width=\"2px\" d=\"M270,102.0 C270,52.0 345.0,52.0 345.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9aef1e1900cf4676b1de52c7f678fb30-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M345.0,104.0 L353.0,92.0 337.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9aef1e1900cf4676b1de52c7f678fb30-0-3\" stroke-width=\"2px\" d=\"M370,102.0 C370,52.0 445.0,52.0 445.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9aef1e1900cf4676b1de52c7f678fb30-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M445.0,104.0 L453.0,92.0 437.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9aef1e1900cf4676b1de52c7f678fb30-0-4\" stroke-width=\"2px\" d=\"M570,102.0 C570,52.0 645.0,52.0 645.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9aef1e1900cf4676b1de52c7f678fb30-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570,104.0 L562,92.0 578,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9aef1e1900cf4676b1de52c7f678fb30-0-5\" stroke-width=\"2px\" d=\"M670,102.0 C670,52.0 745.0,52.0 745.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9aef1e1900cf4676b1de52c7f678fb30-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,104.0 L662,92.0 678,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9aef1e1900cf4676b1de52c7f678fb30-0-6\" stroke-width=\"2px\" d=\"M470,102.0 C470,2.0 750.0,2.0 750.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9aef1e1900cf4676b1de52c7f678fb30-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,104.0 L758.0,92.0 742.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text for dependency parsing\n",
    "about_interest_text = \"He is interested in learning Natural Language Processing.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "about_interest_doc = nlp(about_interest_text)\n",
    "\n",
    "# Generate HTML code for the dependency parse tree\n",
    "html_code = displacy.render(about_interest_doc, style=\"dep\", options={\"distance\": 100})\n",
    "\n",
    "# Print or save the HTML code\n",
    "print(html_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Processing Functions**\n",
    "<font color = \"orange\">\n",
    "\n",
    "## In NLP, processing functions refer to operations performed on text to prepare it for analysis. Common functions include:\n",
    "\n",
    "<font color = \"lightblue\">\n",
    "\n",
    "- ### Tokenization: Splitting text into individual tokens (words or phrases).\n",
    "- ### Lemmatization: Reducing words to their base form.\n",
    "- ### Stop Word Removal: Filtering out frequently used words with little meaning.\n",
    "- ### POS Tagging: Assigning grammatical tags to each word.\n",
    "- ### Named Entity Recognition (NER): Identifying entities like names, dates, and places.\n",
    "- ### Text Normalization: Converting text to a standard format (e.g., lowercase, removing special characters).\n",
    "- ### Dependency Parsing: Analyzing relationships between words in a sentence.\n",
    "<font color = \"snow\">\n",
    "\n",
    "### *These functions are foundational steps in text preprocessing for tasks like sentiment analysis, summarization, or machine translation.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gus', 'proto', 'python', 'developer', 'currently', 'work', 'london', 'base', 'fintech', 'company', 'interested', 'learn', 'natural', 'language', 'processing', 'developer', 'conference', 'happen', '21', 'july', '2019', 'london', 'title', 'application', 'natural', 'language', 'processing', 'helpline', 'number', 'available', '+44', '1234567891', 'gus', 'helping', 'organize', 'keep', 'organize', 'local', 'python', 'meetup', 'internal', 'talk', 'workplace', 'gus', 'present', 'talk', 'talk', 'introduce', 'reader', 'use', 'case', 'natural', 'language', 'processing', 'fintech', 'apart', 'work', 'passionate', 'music', 'gus', 'learn', 'play', 'piano', 'enrol', 'weekend', 'batch', 'great', 'piano', 'academy', 'great', 'piano', 'academy', 'situate', 'mayfair', 'city', 'london', 'world', 'class', 'piano', 'instructor']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text\n",
    "complete_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech company. He is\"\n",
    "    \" interested in learning Natural Language Processing.\"\n",
    "    \" There is a developer conference happening on 21 July\"\n",
    "    ' 2019 in London. It is titled \"Applications of Natural'\n",
    "    ' Language Processing\". There is a helpline number'\n",
    "    \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "    \" He keeps organizing local Python meetups and several\"\n",
    "    \" internal talks at his workplace. Gus is also presenting\"\n",
    "    ' a talk. The talk will introduce the reader about \"Use'\n",
    "    ' cases of Natural Language Processing in Fintech\".'\n",
    "    \" Apart from his work, he is very passionate about music.\"\n",
    "    \" Gus is learning to play the Piano. He has enrolled\"\n",
    "    \" himself in the weekend batch of Great Piano Academy.\"\n",
    "    \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "    \" of London and has world-class piano instructors.\"\n",
    ")\n",
    "\n",
    "# Process the text with spaCy\n",
    "complete_doc = nlp(complete_text)\n",
    "\n",
    "# Define the function to check if a token is allowed\n",
    "def is_token_allowed(token):\n",
    "    return bool(\n",
    "        token\n",
    "        and str(token).strip()\n",
    "        and not token.is_stop\n",
    "        and not token.is_punct\n",
    "    )\n",
    "\n",
    "# Define the function to preprocess the token (lemmatize and lowercase)\n",
    "def preprocess_token(token):\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "# Filter and preprocess tokens\n",
    "complete_filtered_tokens = [\n",
    "    preprocess_token(token)\n",
    "    for token in complete_doc\n",
    "    if is_token_allowed(token)\n",
    "]\n",
    "\n",
    "# Print the filtered tokens\n",
    "print(complete_filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"orange\">\n",
    "\n",
    "# 2.f) Rule based matching using SpaCy\n",
    "<font color = \"snow\">\n",
    "\n",
    "- ### *Rule-based matching is one of the steps in extracting information from unstructured text. It's used to identify and extract tokens and phrases according to patterns (such as lowercase) and grammatical features (such as part of speech).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Proto\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text\n",
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "\n",
    "# Process the text with spaCy\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "# Initialize the matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the function to extract full name\n",
    "def extract_full_name(nlp_doc):\n",
    "    pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}]\n",
    "    matcher.add(\"FULL_NAME\", [pattern])\n",
    "    \n",
    "    # Find the matches\n",
    "    matches = matcher(nlp_doc)\n",
    "    \n",
    "    # Extract full names from matches\n",
    "    for _, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        yield span.text\n",
    "\n",
    "# Print the extracted full name\n",
    "print(next(extract_full_name(about_doc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color = \"orange\">\n",
    "\n",
    "# 2.g) Dependency Parsing Using spaCy\n",
    "\n",
    "<font color = \"snow\">\n",
    "\n",
    "- ### *Dependency Parsing in spaCy analyzes the grammatical structure of a sentence by identifying the relationships between words, such as which words are subjects, objects, or modifiers. It helps in understanding how words in a sentence are connected.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN: Gus\n",
      "TAG: NNP\n",
      "HEAD: learning\n",
      "DEP: nsubj\n",
      "------\n",
      "TOKEN: is\n",
      "TAG: VBZ\n",
      "HEAD: learning\n",
      "DEP: aux\n",
      "------\n",
      "TOKEN: learning\n",
      "TAG: VBG\n",
      "HEAD: learning\n",
      "DEP: ROOT\n",
      "------\n",
      "TOKEN: piano\n",
      "TAG: NN\n",
      "HEAD: learning\n",
      "DEP: dobj\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "piano_text = \"Gus is learning piano\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "piano_doc = nlp(piano_text)\n",
    "\n",
    "# Loop through the tokens and print their details\n",
    "for token in piano_doc:\n",
    "    print(f\"TOKEN: {token.text}\")\n",
    "    print(f\"TAG: {token.tag_}\")\n",
    "    print(f\"HEAD: {token.head.text}\")\n",
    "    print(f\"DEP: {token.dep_}\")\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sudeesh Kumar V\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\displacy\\__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"83b5a58fab9f434f90c543840c0d4386-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Gus</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">piano</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-83b5a58fab9f434f90c543840c0d4386-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-83b5a58fab9f434f90c543840c0d4386-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-83b5a58fab9f434f90c543840c0d4386-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-83b5a58fab9f434f90c543840c0d4386-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-83b5a58fab9f434f90c543840c0d4386-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-83b5a58fab9f434f90c543840c0d4386-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(piano_doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'Python', 'working']\n",
      "Python\n",
      "currently\n",
      "['a', 'Python']\n",
      "['working']\n",
      "['a', 'Python', 'developer', 'currently', 'working', 'for', 'a', 'London', '-', 'based', 'Fintech', 'company']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "one_line_about_text = (\n",
    "    \"Gus Proto is a Python developer\"\n",
    "    \" currently working for a London-based Fintech company\"\n",
    ")\n",
    "\n",
    "# Process the text with spaCy\n",
    "one_line_about_doc = nlp(one_line_about_text)\n",
    "\n",
    "# Extract children of 'developer'\n",
    "print([token.text for token in one_line_about_doc[5].children])\n",
    "\n",
    "# Extract previous neighboring node of 'developer'\n",
    "print(one_line_about_doc[5].nbor(-1))\n",
    "\n",
    "# Extract next neighboring node of 'developer'\n",
    "print(one_line_about_doc[5].nbor())\n",
    "\n",
    "# Extract all tokens on the left of 'developer'\n",
    "print([token.text for token in one_line_about_doc[5].lefts])\n",
    "\n",
    "# Extract tokens on the right of 'developer'\n",
    "print([token.text for token in one_line_about_doc[5].rights])\n",
    "\n",
    "# Print subtree of 'developer'\n",
    "print([token.text for token in one_line_about_doc[5].subtree])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"orange\">\n",
    "\n",
    "# Named-Entity Recognition\n",
    "\n",
    "<font color = \"snow\">\n",
    "\n",
    "- ### *Named Entity Recognition (NER) is an NLP task that identifies and classifies entities in text into categories like names, locations, dates, and organizations. It helps in extracting structured information from unstructured text for various applications like search engines and document processing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Great Piano Academy\n",
      "Start character: 0\n",
      "End character: 19\n",
      "Label: ORG\n",
      "Explanation: Companies, agencies, institutions, etc.\n",
      "\n",
      "Text: Mayfair\n",
      "Start character: 35\n",
      "End character: 42\n",
      "Label: FAC\n",
      "Explanation: Buildings, airports, highways, bridges, etc.\n",
      "\n",
      "Text: the City of London\n",
      "Start character: 46\n",
      "End character: 64\n",
      "Label: GPE\n",
      "Explanation: Countries, cities, states\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "piano_class_text = (\n",
    "    \"Great Piano Academy is situated\"\n",
    "    \" in Mayfair or the City of London and has\"\n",
    "    \" world-class piano instructors.\"\n",
    ")\n",
    "\n",
    "# Process the text with spaCy\n",
    "piano_class_doc = nlp(piano_class_text)\n",
    "\n",
    "# Iterate over the named entities and print relevant information\n",
    "for ent in piano_class_doc.ents:\n",
    "    print(f\"Text: {ent.text}\")\n",
    "    print(f\"Start character: {ent.start_char}\")\n",
    "    print(f\"End character: {ent.end_char}\")\n",
    "    print(f\"Label: {ent.label_}\")\n",
    "    print(f\"Explanation: {spacy.explain(ent.label_)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Great Piano Academy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is situated in \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mayfair\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the City of London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and has world-class piano instructors.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(piano_class_doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 5 people surveyed, [REDACTED], [REDACTED]and [REDACTED]like apples. [REDACTED]and [REDACTED]like oranges.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Survey text\n",
    "survey_text = (\n",
    "    \"Out of 5 people surveyed, James Robert,\"\n",
    "    \" Julie Fuller and Benjamin Brooks like\"\n",
    "    \" apples. Kelly Cox and Matthew Evans\"\n",
    "    \" like oranges.\"\n",
    ")\n",
    "\n",
    "# Function to replace person names with \"[REDACTED]\"\n",
    "def replace_person_names(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
    "        return \"[REDACTED]\"\n",
    "    return token.text_with_ws\n",
    "\n",
    "# Function to redact names in the document\n",
    "def redact_names(nlp_doc):\n",
    "    # Using retokenizer to merge entities\n",
    "    with nlp_doc.retokenize() as retokenizer:\n",
    "        for ent in nlp_doc.ents:\n",
    "            retokenizer.merge(ent)\n",
    "    \n",
    "    # Map tokens to replace names\n",
    "    tokens = map(replace_person_names, nlp_doc)\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    return \"\".join(tokens)\n",
    "\n",
    "# Process the text with spaCy\n",
    "survey_doc = nlp(survey_text)\n",
    "\n",
    "# Print the redacted survey text\n",
    "print(redact_names(survey_doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"orange\">\n",
    "\n",
    "# **INFERENCE**\n",
    "<font color = \"snow\">\n",
    "\n",
    "\n",
    "- **Sentence Detection**: We implemented custom rules for segmenting text into sentences, demonstrating spaCy's ability to detect sentence boundaries effectively.\n",
    "\n",
    "- **Stop Words Removal**: The lab showcased how stop words (common words with little meaning) can be filtered out to enhance data quality and focus on more meaningful words.\n",
    "\n",
    "- **Lemmatization**: We explored how spaCy’s lemmatization function reduces words to their base form, improving consistency and simplifying text analysis.\n",
    "\n",
    "- **Word Frequency Analysis**: We analyzed the frequency of words in a text to identify common terms, aiding in understanding the text's primary themes.\n",
    "\n",
    "- **Visualization using displaCy**: We used spaCy’s displaCy tool to create interactive visualizations of sentence structures and named entities, helping to visualize dependencies and relationships in text.\n",
    "\n",
    "- **Dependency Parsing**: We examined how spaCy's dependency parsing reveals the syntactic relationships between words in a sentence, providing insights into the grammatical structure.\n",
    "\n",
    "- **Rule-Based Matching**: We applied rule-based matching to detect specific patterns in the text, showcasing spaCy's flexibility for custom text extraction tasks.\n",
    "\n",
    "<font color = \"Orange\">\n",
    "\n",
    "### Summary\n",
    "<font color = \"snow\">\n",
    "\n",
    "- ### *In this NLP lab, we explored the core functionalities of spaCy to process and analyze text. We started with sentence detection, where we implemented custom rules to accurately segment text into sentences. The lab also covered stop words removal and lemmatization, demonstrating techniques to clean and normalize text. We performed word frequency analysis to examine the most common terms in a corpus, followed by part-of-speech tagging to identify grammatical categories. The displaCy tool was used for visualization, helping us better understand sentence structure and named entities. Dependency parsing allowed us to uncover the syntactic relationships between words, providing deeper insights into sentence construction. Additionally, rule-based matching was introduced to extract specific patterns from text. Overall, the lab provided a hands-on understanding of spaCy’s robust features for text processing and analysis in natural language processing.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
